{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Experiments - LLM-GM System Evaluation\n",
    "\n",
    "This Notebook is used to evaluate the performance of the RAG (Retrieval-Augmented Generation) system.\n",
    "\n",
    "## Experiment Objectives\n",
    "1. **Baseline Comparison**: Compare three configurations: No RAG, Random Sampling, and RAG\n",
    "2. **Ablation Studies**: Test the impact of different parameters (threshold, TOP_K, temperature)\n",
    "3. **Statistical Analysis**: Calculate significance and confidence intervals\n",
    "4. **Result Visualization**: Generate plots and tables\n",
    "\n",
    "## Experiment Configuration\n",
    "- Test World: RAG Test World\n",
    "- Runs per configuration: 3runs\n",
    "- Test scenarios: 10different types of player actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"âœ… Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import configuration and utility modules\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from config import (\n",
    "    SUPABASE_URL, SUPABASE_ANON_KEY, OPENAI_API_KEY,\n",
    "    TEST_WORLD_NAME, RUNS_PER_CONFIG\n",
    ")\n",
    "from utils.rag_simulator import RAGSimulator\n",
    "from utils.metrics import (\n",
    "    aggregate_metrics, compare_baselines, perform_t_test,\n",
    "    calculate_confidence_interval, format_metrics_table,\n",
    "    calculate_cost_savings\n",
    ")\n",
    "\n",
    "# Ensure results directory exists\n",
    "os.makedirs('results', exist_ok=True)\n",
    "os.makedirs('results/plots', exist_ok=True)\n",
    "\n",
    "print(\"âœ… Configuration and utility modules imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Supabase and OpenAI clients\n",
    "from supabase import create_client\n",
    "from openai import OpenAI\n",
    "\n",
    "supabase = create_client(SUPABASE_URL, SUPABASE_ANON_KEY)\n",
    "openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "print(\"âœ… API clients initialized successfully\")\n",
    "print(f\"Supabase URL: {SUPABASE_URL}\")\n",
    "print(f\"OpenAI Model: gpt-4.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test scenarios\n",
    "with open('test_scenarios.json', 'r', encoding='utf-8') as f:\n",
    "    scenarios_data = json.load(f)\n",
    "    test_scenarios = scenarios_data['scenarios']\n",
    "\n",
    "print(f\"âœ… Loaded {len(test_scenarios)} test scenarios\")\n",
    "print(\"\\nTest scenario list:\")\n",
    "for scenario in test_scenarios:\n",
    "    print(f\"  {scenario['id']}. [{scenario['category']}] {scenario['player_message']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get test world ID\n",
    "world_response = supabase.table('worlds')\\\n",
    "    .select('id, name, description')\\\n",
    "    .eq('name', TEST_WORLD_NAME)\\\n",
    "    .single()\\\n",
    "    .execute()\n",
    "\n",
    "test_world = world_response.data\n",
    "WORLD_ID = test_world['id']\n",
    "\n",
    "print(f\"âœ… Found test world: {test_world['name']}\")\n",
    "print(f\"World ID: {WORLD_ID}\")\n",
    "print(f\"Description: {test_world['description']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check entity counts in the world\n",
    "entity_types = ['items', 'abilities', 'locations', 'npcs', 'organizations', 'taxonomies', 'rules']\n",
    "entity_counts = {}\n",
    "\n",
    "for entity_type in entity_types:\n",
    "    response = supabase.table(entity_type)\\\n",
    "        .select('id', count='exact')\\\n",
    "        .eq('world_id', WORLD_ID)\\\n",
    "        .execute()\n",
    "    entity_counts[entity_type] = response.count\n",
    "\n",
    "print(\"\\nEntity statistics:\")\n",
    "for entity_type, count in entity_counts.items():\n",
    "    print(f\"  {entity_type}: {count}\")\n",
    "print(f\"\\nTotal entities: {sum(entity_counts.values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for saving/loading experiment data\n",
    "def save_experiment_results(data, filename):\n",
    "    \"\"\"Save experiment results to JSON file\"\"\"\n",
    "    filepath = f'results/{filename}'\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2, default=str)\n",
    "    print(f\"âœ… Experiment results saved to: {filepath}\")\n",
    "    print(f\"   Save time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    return filepath\n",
    "\n",
    "def load_experiment_results(filename):\n",
    "    \"\"\"Load experiment results from JSON file\"\"\"\n",
    "    filepath = f'results/{filename}'\n",
    "    if os.path.exists(filepath):\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        print(f\"âœ… Loaded from backup: {filepath}\")\n",
    "        return data\n",
    "    else:\n",
    "        print(f\"NO BACKUP FILES: {filepath}\")\n",
    "        return None\n",
    "\n",
    "print(\"âœ… Helper functions for experiment data save/load defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RAG simulator\n",
    "simulator = RAGSimulator(supabase, openai_client, WORLD_ID)\n",
    "print(\"âœ… RAG simulator initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Baseline Comparison Experiment\n",
    "\n",
    "Compare three configurations:\n",
    "1. **No RAG**: Pass all world entities\n",
    "2. **Random Sampling**: Randomly select k entities\n",
    "3. **RAG**: Semantic similarity retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run baseline comparison experiment\n",
    "baseline_results = {\n",
    "    'no_rag': [],\n",
    "    'random': [],\n",
    "    'rag': []\n",
    "}\n",
    "\n",
    "print(\"Starting baseline comparison experiment...\")\n",
    "print(f\"Runs per configuration {RUNS_PER_CONFIG} runs, in total {len(test_scenarios)} scenarios\\n\")\n",
    "\n",
    "total_runs = len(test_scenarios) * RUNS_PER_CONFIG * 3\n",
    "pbar = tqdm(total=total_runs, desc=\"Overall progress\")\n",
    "\n",
    "for scenario in test_scenarios:\n",
    "    player_message = scenario['player_message']\n",
    "    \n",
    "    for run in range(RUNS_PER_CONFIG):\n",
    "        # No RAG\n",
    "        result = simulator.run_experiment(\n",
    "            player_message=player_message,\n",
    "            mode='no_rag'\n",
    "        )\n",
    "        result['scenario_id'] = scenario['id']\n",
    "        result['run'] = run + 1\n",
    "        baseline_results['no_rag'].append(result)\n",
    "        pbar.update(1)\n",
    "        time.sleep(0.5)  # Avoid API rate limiting\n",
    "        \n",
    "        # Random Sampling\n",
    "        result = simulator.run_experiment(\n",
    "            player_message=player_message,\n",
    "            mode='random',\n",
    "            top_k=5\n",
    "        )\n",
    "        result['scenario_id'] = scenario['id']\n",
    "        result['run'] = run + 1\n",
    "        baseline_results['random'].append(result)\n",
    "        pbar.update(1)\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "        # RAG\n",
    "        result = simulator.run_experiment(\n",
    "            player_message=player_message,\n",
    "            mode='rag',\n",
    "            top_k=5,\n",
    "            similarity_threshold=0.65\n",
    "        )\n",
    "        result['scenario_id'] = scenario['id']\n",
    "        result['run'] = run + 1\n",
    "        baseline_results['rag'].append(result)\n",
    "        pbar.update(1)\n",
    "        time.sleep(0.5)\n",
    "\n",
    "pbar.close()\n",
    "print(\"\\nâœ… Baseline comparison experiment completed!\")\n",
    "\n",
    "# Save baseline comparison results immediately\n",
    "save_experiment_results(baseline_results, 'baseline_results_backup.json')\n",
    "print(f\"\\nData statistics:\")\n",
    "print(f\"   - No RAG: {len(baseline_results['no_rag'])} results\")\n",
    "print(f\"   - Random: {len(baseline_results['random'])} results\")\n",
    "print(f\"   - RAG: {len(baseline_results['rag'])} results\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check and load baseline comparison results (if not in memory)\n",
    "if 'baseline_results' not in locals() or not baseline_results.get('no_rag'):\n",
    "    print(\"âš ï¸ baseline_results does not exist or is empty, trying to load from backup...\")\n",
    "    loaded_data = load_experiment_results('baseline_results_backup.json')\n",
    "    if loaded_data:\n",
    "        baseline_results = loaded_data\n",
    "    else:\n",
    "        print(\"âŒ Backup file does not exist, please run first Cell 11ï¼ˆBaseline Comparison Experimentï¼‰\")\n",
    "        raise FileNotFoundError(\"baseline_results does not exist and no backup file\")\n",
    "else:\n",
    "    print(\"âœ… baseline_results already exists in memory\")\n",
    "\n",
    "# Check data integrity\n",
    "print(f\"\\nData check:\")\n",
    "print(f\"  No RAG: {len(baseline_results.get('no_rag', []))} entries\")\n",
    "print(f\"  Random: {len(baseline_results.get('random', []))} entries\")\n",
    "print(f\"  RAG: {len(baseline_results.get('rag', []))} entries\")\n",
    "\n",
    "if len(baseline_results.get('no_rag', [])) == 0:\n",
    "    print(\"âš ï¸ Warning: baseline_results is empty, please run first Cell 11\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check baseline_results exists, if not try to load from backup\n",
    "import json\n",
    "import os\n",
    "\n",
    "if 'baseline_results' not in locals() or not baseline_results.get('no_rag'):\n",
    "    print(\"âš ï¸ baseline_results does not exist or is empty, trying to load from backup...\")\n",
    "    backup_file = 'results/baseline_results_backup.json'\n",
    "    if os.path.exists(backup_file):\n",
    "        with open(backup_file, 'r', encoding='utf-8') as f:\n",
    "            baseline_results = json.load(f)\n",
    "        print(f\"âœ… Loaded from backup: {backup_file}\")\n",
    "    else:\n",
    "        print(\"âŒ Backup file does not exist, please run first Cell 11ï¼ˆBaseline Comparison Experimentï¼‰\")\n",
    "        raise FileNotFoundError(\"baseline_results does not exist and no backup file\")\n",
    "else:\n",
    "    print(\"âœ… baseline_results already exists in memory\")\n",
    "\n",
    "# Check data integrity\n",
    "print(f\"\\nData check:\")\n",
    "print(f\"  No RAG: {len(baseline_results.get('no_rag', []))} entries\")\n",
    "print(f\"  Random: {len(baseline_results.get('random', []))} entries\")\n",
    "print(f\"  RAG: {len(baseline_results.get('rag', []))} entries\")\n",
    "\n",
    "if len(baseline_results.get('no_rag', [])) == 0:\n",
    "    print(\"âš ï¸ Warning: baseline_results is empty, please run first Cell 11\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze baseline comparison results\n",
    "baseline_comparison = compare_baselines(\n",
    "    baseline_results['no_rag'],\n",
    "    baseline_results['random'],\n",
    "    baseline_results['rag']\n",
    ")\n",
    "\n",
    "print(\"=== Baseline Comparison Results ===\")\n",
    "print(f\"\\nContext Efficiency: {baseline_comparison.get('context_efficiency_percentage', 0):.2f}% reduction\")\n",
    "\n",
    "print(\"\\n--- No RAG ---\")\n",
    "print(f\"Average input tokens: {baseline_comparison['no_rag']['input_tokens']['mean']:.0f}\")\n",
    "print(f\"Average latency: {baseline_comparison['no_rag']['latency']['mean']:.2f}s\")\n",
    "print(f\"Average cost: ${baseline_comparison['no_rag']['cost']['mean']:.4f}\")\n",
    "\n",
    "print(\"\\n--- Random Sampling ---\")\n",
    "print(f\"Average input tokens: {baseline_comparison['random']['input_tokens']['mean']:.0f}\")\n",
    "print(f\"Average latency: {baseline_comparison['random']['latency']['mean']:.2f}s\")\n",
    "print(f\"Average cost: ${baseline_comparison['random']['cost']['mean']:.4f}\")\n",
    "\n",
    "print(\"\\n--- RAG ---\")\n",
    "print(f\"Average input tokens: {baseline_comparison['rag']['input_tokens']['mean']:.0f}\")\n",
    "print(f\"Average latency: {baseline_comparison['rag']['latency']['mean']:.2f}s\")\n",
    "print(f\"Average cost: ${baseline_comparison['rag']['cost']['mean']:.4f}\")\n",
    "\n",
    "print(\"\\n--- Statistical Significance ---\")\n",
    "for test_name, test_result in baseline_comparison['significance_tests'].items():\n",
    "    sig = \"âœ… Significant\" if test_result['significant'] else \"âŒ Not significant\"\n",
    "    print(f\"{test_name}: p={test_result['p_value']:.4f} {sig}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize baseline comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "configs = ['no_rag', 'random', 'rag']\n",
    "config_labels = ['No RAG', 'Random', 'RAG']\n",
    "\n",
    "# 1. Input tokens comparison\n",
    "input_tokens = [\n",
    "    baseline_comparison[c]['input_tokens']['mean']\n",
    "    for c in configs\n",
    "]\n",
    "axes[0, 0].bar(config_labels, input_tokens, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "axes[0, 0].set_ylabel('Average Input Tokens')\n",
    "axes[0, 0].set_title('Input Tokens Comparison')\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 2. Latency comparison\n",
    "latencies = [\n",
    "    baseline_comparison[c]['latency']['mean']\n",
    "    for c in configs\n",
    "]\n",
    "axes[0, 1].bar(config_labels, latencies, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "axes[0, 1].set_ylabel('Average Latency (seconds)')\n",
    "axes[0, 1].set_title('Latency Comparison')\n",
    "axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 3. Cost comparison\n",
    "costs = [\n",
    "    baseline_comparison[c]['cost']['mean']\n",
    "    for c in configs\n",
    "]\n",
    "axes[1, 0].bar(config_labels, costs, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "axes[1, 0].set_ylabel('Average Cost ($)')\n",
    "axes[1, 0].set_title('Cost Comparison')\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 4. Token distribution box plot\n",
    "all_input_tokens = [\n",
    "    [r['input_tokens'] for r in baseline_results[c]]\n",
    "    for c in configs\n",
    "]\n",
    "axes[1, 1].boxplot(all_input_tokens, labels=config_labels)\n",
    "axes[1, 1].set_ylabel('Input Tokens')\n",
    "axes[1, 1].set_title('Input Tokens Distribution')\n",
    "axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/plots/baseline_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Plot saved to results/plots/baseline_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ablation Studies\n",
    "\n",
    "### 4.1 RAG Threshold Ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG Threshold ablation experiment\n",
    "threshold_values = [0.5, 0.65, 0.8]\n",
    "\n",
    "\n",
    "print(\"Check if there are saved Threshold ablation experiment results...\")\n",
    "loaded_threshold = load_experiment_results('threshold_results_backup.json')\n",
    "if loaded_threshold:\n",
    "    # Fix: Convert string keys to float keys\n",
    "    threshold_results = {float(k): v for k, v in loaded_threshold.items()}\n",
    "    print(f\"âœ… Loaded saved results, containing {len(threshold_results)}  threshold's experiment data\")\n",
    "    missing_thresholds = [t for t in threshold_values if not threshold_results.get(t) or len(threshold_results[t]) == 0]\n",
    "    if missing_thresholds:\n",
    "        print(f\"âš ï¸ The following threshold data is missing or empty: {missing_thresholds}\")\n",
    "        print(\"Will continue to run experiments for these thresholds...\")\n",
    "else:\n",
    "    threshold_results = {t: [] for t in threshold_values}\n",
    "\n",
    "# Use only the first 5 scenarios to save time\n",
    "if 'ablation_scenarios' not in locals():\n",
    "    ablation_scenarios = test_scenarios[:5]\n",
    "\n",
    "print(\"\\nStarting RAG Threshold ablation experiment...\")\n",
    "print(f\"Test thresholds: {threshold_values}\\n\")\n",
    "\n",
    "for threshold in threshold_values:\n",
    "    # Skip if this threshold already has complete data\n",
    "    if threshold_results.get(threshold) and len(threshold_results[threshold]) > 0:\n",
    "        expected_count = len(ablation_scenarios) * RUNS_PER_CONFIG\n",
    "        if len(threshold_results[threshold]) >= expected_count:\n",
    "            print(f\"\\nSkip threshold={threshold} (already has {len(threshold_results[threshold])} data entries)\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nTesting threshold={threshold}\")\n",
    "    pbar = tqdm(total=len(ablation_scenarios) * RUNS_PER_CONFIG)\n",
    "    \n",
    "    for scenario in ablation_scenarios:\n",
    "        for run in range(RUNS_PER_CONFIG):\n",
    "            result = simulator.run_experiment(\n",
    "                player_message=scenario['player_message'],\n",
    "                mode='rag',\n",
    "                similarity_threshold=threshold\n",
    "            )\n",
    "            result['scenario_id'] = scenario['id']\n",
    "            result['run'] = run + 1\n",
    "            threshold_results[threshold].append(result)\n",
    "            pbar.update(1)\n",
    "            time.sleep(0.3)\n",
    "    \n",
    "    pbar.close()\n",
    "    \n",
    "    # Save immediately after each threshold is completed\n",
    "    save_experiment_results(threshold_results, 'threshold_results_backup.json')\n",
    "\n",
    "print(\"\\nâœ… RAG Threshold ablation experiment finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Threshold Ablation Result\n",
    "threshold_comparison = {}\n",
    "for threshold in threshold_values:\n",
    "    threshold_comparison[threshold] = aggregate_metrics(threshold_results[threshold])\n",
    "\n",
    "print(\"=== RAG Threshold Ablation Result ===\")\n",
    "for threshold in threshold_values:\n",
    "    metrics = threshold_comparison[threshold]\n",
    "    print(f\"\\nThreshold = {threshold}:\")\n",
    "    print(f\"  Average retrieved entities: {np.mean([r['total_entities_retrieved'] for r in threshold_results[threshold]]):.1f}\")\n",
    "    print(f\"  Average input tokens: {metrics['input_tokens']['mean']:.0f}\")\n",
    "    print(f\"  Average latency: {metrics['latency']['mean']:.2f}s\")\n",
    "    print(f\"  Average cost: ${metrics['cost']['mean']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Threshold impact\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Retrieved entities\n",
    "entity_counts = [\n",
    "    np.mean([r['total_entities_retrieved'] for r in threshold_results[t]])\n",
    "    for t in threshold_values\n",
    "]\n",
    "axes[0].plot(threshold_values, entity_counts, marker='o', linewidth=2, markersize=8)\n",
    "axes[0].set_xlabel('Similarity Threshold')\n",
    "axes[0].set_ylabel('Avg Retrieved Entities')\n",
    "axes[0].set_title('Retrieved Entities vs Threshold')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Input Tokens\n",
    "input_tokens = [\n",
    "    threshold_comparison[t]['input_tokens']['mean']\n",
    "    for t in threshold_values\n",
    "]\n",
    "axes[1].plot(threshold_values, input_tokens, marker='s', linewidth=2, markersize=8, color='orange')\n",
    "axes[1].set_xlabel('Similarity Threshold')\n",
    "axes[1].set_ylabel('Avg Input Tokens')\n",
    "axes[1].set_title('Input Tokens vs Threshold')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# Costs\n",
    "costs = [\n",
    "    threshold_comparison[t]['cost']['mean']\n",
    "    for t in threshold_values\n",
    "]\n",
    "axes[2].plot(threshold_values, costs, marker='^', linewidth=2, markersize=8, color='green')\n",
    "axes[2].set_xlabel('Similarity Threshold')\n",
    "axes[2].set_ylabel('Avg Cost ($)')\n",
    "axes[2].set_title('Cost vs Threshold')\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/plots/ablation_threshold.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Plot saved to results/plots/ablation_threshold.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 TOP_K Ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOP_K ablation experiment\n",
    "top_k_values = [3, 5, 10]\n",
    "\n",
    "\n",
    "print(\"Check if there are saved TOP_K ablation experiment result...\")\n",
    "loaded_top_k = load_experiment_results('top_k_results_backup.json')\n",
    "if loaded_top_k:\n",
    "    top_k_results = {int(k): v for k, v in loaded_top_k.items()}\n",
    "    print(f\"âœ… Loaded saved results, containing {len(top_k_results)} TOP_K value experiment data\")\n",
    "    missing_top_k = [k for k in top_k_values if not top_k_results.get(k) or len(top_k_results[k]) == 0]\n",
    "    if missing_top_k:\n",
    "        print(f\"âš ï¸ The following TOP_K values data is missing or empty: {missing_top_k}\")\n",
    "        print(\"Will continue to run experiments for these TOP_K values...\")\n",
    "else:\n",
    "    top_k_results = {k: [] for k in top_k_values}\n",
    "\n",
    "print(\"\\nStarting TOP_K ablation experiment...\")\n",
    "print(f\"Test TOP_K values: {top_k_values}\\n\")\n",
    "\n",
    "for top_k in top_k_values:\n",
    "    # Skip if this TOP_K already has complete data\n",
    "    if top_k_results.get(top_k) and len(top_k_results[top_k]) > 0:\n",
    "        expected_count = len(ablation_scenarios) * RUNS_PER_CONFIG\n",
    "        if len(top_k_results[top_k]) >= expected_count:\n",
    "            print(f\"\\nSkip TOP_K={top_k} (already has {len(top_k_results[top_k])} data entries)\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nTesting TOP_K={top_k}\")\n",
    "    pbar = tqdm(total=len(ablation_scenarios) * RUNS_PER_CONFIG)\n",
    "    \n",
    "    for scenario in ablation_scenarios:\n",
    "        for run in range(RUNS_PER_CONFIG):\n",
    "            result = simulator.run_experiment(\n",
    "                player_message=scenario['player_message'],\n",
    "                mode='rag',\n",
    "                top_k=top_k\n",
    "            )\n",
    "            result['scenario_id'] = scenario['id']\n",
    "            result['run'] = run + 1\n",
    "            top_k_results[top_k].append(result)\n",
    "            pbar.update(1)\n",
    "            time.sleep(0.3)\n",
    "    \n",
    "    pbar.close()\n",
    "    \n",
    "    # Save immediately after each TOP_K is completed\n",
    "    save_experiment_results(top_k_results, 'top_k_results_backup.json')\n",
    "\n",
    "print(\"\\nâœ… TOP_K ablation experiment Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze TOP_K Ablation Result\n",
    "top_k_comparison = {}\n",
    "for k in top_k_values:\n",
    "    top_k_comparison[k] = aggregate_metrics(top_k_results[k])\n",
    "\n",
    "print(\"=== TOP_K ablation results ===\")\n",
    "for k in top_k_values:\n",
    "    metrics = top_k_comparison[k]\n",
    "    print(f\"\\nTOP_K = {k}:\")\n",
    "    print(f\"  Average input tokens: {metrics['input_tokens']['mean']:.0f}\")\n",
    "    print(f\"  Average latency: {metrics['latency']['mean']:.2f}s\")\n",
    "    print(f\"  Average cost: ${metrics['cost']['mean']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Temperature Ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature ablation experiment\n",
    "temperature_values = [0.5, 0.8, 1.0]\n",
    "\n",
    "print(\"Check if there are saved Temperature ablation experiment results...\")\n",
    "loaded_temperature = load_experiment_results('temperature_results_backup.json')\n",
    "if loaded_temperature:\n",
    "    # Fix: Convert string keys to float keys\n",
    "    temperature_results = {float(k): v for k, v in loaded_temperature.items()}\n",
    "    print(f\"âœ… Loaded saved results, containing {len(temperature_results)} ä¸ªTemperature value experiment data\")\n",
    "    missing_temp = [t for t in temperature_values if not temperature_results.get(t) or len(temperature_results[t]) == 0]\n",
    "    if missing_temp:\n",
    "        print(f\"âš ï¸ The following Temperature values data is missing or empty: {missing_temp}\")\n",
    "        print(\"Will continue to run experiments for these Temperature values...\")\n",
    "else:\n",
    "    temperature_results = {t: [] for t in temperature_values}\n",
    "\n",
    "print(\"\\nStarting Temperature ablation experiment...\")\n",
    "print(f\"Test Temperature values: {temperature_values}\\n\")\n",
    "\n",
    "for temp in temperature_values:\n",
    "    # Skip if this Temperature already has complete data\n",
    "    if temperature_results.get(temp) and len(temperature_results[temp]) > 0:\n",
    "        expected_count = len(ablation_scenarios) * RUNS_PER_CONFIG\n",
    "        if len(temperature_results[temp]) >= expected_count:\n",
    "            print(f\"\\nSkip Temperature={temp} (already has {len(temperature_results[temp])} data entries)\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nTesting Temperature={temp}\")\n",
    "    pbar = tqdm(total=len(ablation_scenarios) * RUNS_PER_CONFIG)\n",
    "    \n",
    "    for scenario in ablation_scenarios:\n",
    "        for run in range(RUNS_PER_CONFIG):\n",
    "            result = simulator.run_experiment(\n",
    "                player_message=scenario['player_message'],\n",
    "                mode='rag',\n",
    "                temperature=temp\n",
    "            )\n",
    "            result['scenario_id'] = scenario['id']\n",
    "            result['run'] = run + 1\n",
    "            temperature_results[temp].append(result)\n",
    "            pbar.update(1)\n",
    "            time.sleep(0.3)\n",
    "    \n",
    "    pbar.close()\n",
    "    \n",
    "    # Save immediately after each Temperature is completed\n",
    "    save_experiment_results(temperature_results, 'temperature_results_backup.json')\n",
    "\n",
    "print(\"\\nâœ… Temperature ablation experiment finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Temperature Ablation Result\n",
    "temperature_comparison = {}\n",
    "for temp in temperature_values:\n",
    "    temperature_comparison[temp] = aggregate_metrics(temperature_results[temp])\n",
    "\n",
    "print(\"=== Temperature Ablation Result ===\")\n",
    "for temp in temperature_values:\n",
    "    metrics = temperature_comparison[temp]\n",
    "    print(f\"\\nTemperature = {temp}:\")\n",
    "    print(f\"  Average output tokens: {metrics['output_tokens']['mean']:.0f}\")\n",
    "    print(f\"  Output tokens std dev: {metrics['output_tokens']['std']:.0f}\")\n",
    "    print(f\"  Average latency: {metrics['latency']['mean']:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save Experiment Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame and save\n",
    "import os\n",
    "\n",
    "os.makedirs('results', exist_ok=True)\n",
    "\n",
    "# Baseline Comparison Result\n",
    "baseline_df = pd.DataFrame([\n",
    "    {\n",
    "        'config': config,\n",
    "        'scenario_id': r['scenario_id'],\n",
    "        'run': r['run'],\n",
    "        'input_tokens': r['input_tokens'],\n",
    "        'output_tokens': r['output_tokens'],\n",
    "        'total_tokens': r['total_tokens'],\n",
    "        'latency': r['latency'],\n",
    "        'entities_retrieved': r.get('total_entities_retrieved', 0)\n",
    "    }\n",
    "    for config in ['no_rag', 'random', 'rag']\n",
    "    for r in baseline_results[config]\n",
    "])\n",
    "baseline_df.to_csv('results/baseline_comparison.csv', index=False)\n",
    "print(\"âœ… Baseline comparison results saved to results/baseline_comparison.csv\")\n",
    "\n",
    "# Threshold Ablation Result\n",
    "threshold_df = pd.DataFrame([\n",
    "    {\n",
    "        'threshold': threshold,\n",
    "        'scenario_id': r['scenario_id'],\n",
    "        'run': r['run'],\n",
    "        'input_tokens': r['input_tokens'],\n",
    "        'entities_retrieved': r['total_entities_retrieved']\n",
    "    }\n",
    "    for threshold in threshold_values\n",
    "    for r in threshold_results[threshold]\n",
    "])\n",
    "threshold_df.to_csv('results/ablation_rag_threshold.csv', index=False)\n",
    "print(\"âœ… Threshold ablation results saved to results/ablation_rag_threshold.csv\")\n",
    "\n",
    "# TOP_K Abaltion Result\n",
    "top_k_df = pd.DataFrame([\n",
    "    {\n",
    "        'top_k': k,\n",
    "        'scenario_id': r['scenario_id'],\n",
    "        'run': r['run'],\n",
    "        'input_tokens': r['input_tokens'],\n",
    "        'latency': r['latency']\n",
    "    }\n",
    "    for k in top_k_values\n",
    "    for r in top_k_results[k]\n",
    "])\n",
    "top_k_df.to_csv('results/ablation_top_k.csv', index=False)\n",
    "print(\"âœ… TOP_K ablation results saved to results/ablation_top_k.csv\")\n",
    "\n",
    "# Temperature Abaltion Result\n",
    "temperature_df = pd.DataFrame([\n",
    "    {\n",
    "        'temperature': temp,\n",
    "        'scenario_id': r['scenario_id'],\n",
    "        'run': r['run'],\n",
    "        'output_tokens': r['output_tokens'],\n",
    "        'latency': r['latency']\n",
    "    }\n",
    "    for temp in temperature_values\n",
    "    for r in temperature_results[temp]\n",
    "])\n",
    "temperature_df.to_csv('results/ablation_temperature.csv', index=False)\n",
    "print(\"âœ… Temperature ablation results saved to results/ablation_temperature.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Statistical Analysis Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Statistical Analysis Report\n",
    "report_lines = []\n",
    "report_lines.append(\"# RAG Experiment Statistical Analysis Report\\n\")\n",
    "report_lines.append(f\"Experiment Date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "report_lines.append(f\"Test World: {TEST_WORLD_NAME}\\n\")\n",
    "report_lines.append(f\"Number of test scenarios: {len(test_scenarios)}\\n\")\n",
    "report_lines.append(f\"Runs per configuration: {RUNS_PER_CONFIG}\\n\\n\")\n",
    "\n",
    "report_lines.append(\"## 1. Baseline Comparison Result\\n\\n\")\n",
    "report_lines.append(f\"Context Efficiency: {baseline_comparison.get('context_efficiency_percentage', 0):.2f}% reduction\\n\\n\")\n",
    "\n",
    "for config in ['no_rag', 'random', 'rag']:\n",
    "    metrics = baseline_comparison[config]\n",
    "    report_lines.append(f\"### {config.upper()}\\n\")\n",
    "    report_lines.append(f\"- Input Tokens: {metrics['input_tokens']['mean']:.2f} Â± {metrics['input_tokens']['std']:.2f}\\n\")\n",
    "    report_lines.append(f\"- Latency: {metrics['latency']['mean']:.3f}s Â± {metrics['latency']['std']:.3f}s\\n\")\n",
    "    report_lines.append(f\"- Cost: ${metrics['cost']['mean']:.5f} Â± ${metrics['cost']['std']:.5f}\\n\\n\")\n",
    "\n",
    "report_lines.append(\"## 2. Statistic Significant Testing\\n\\n\")\n",
    "for test_name, test_result in baseline_comparison['significance_tests'].items():\n",
    "    report_lines.append(f\"### {test_name}\\n\")\n",
    "    report_lines.append(f\"- t-statistic: {test_result['t_statistic']:.4f}\\n\")\n",
    "    report_lines.append(f\"- p-value: {test_result['p_value']:.4f}\\n\")\n",
    "    report_lines.append(f\"- Significant (p < 0.05): {test_result['significant']}\\n\\n\")\n",
    "\n",
    "with open('results/statistical_analysis.txt', 'w', encoding='utf-8') as f:\n",
    "    f.writelines(report_lines)\n",
    "\n",
    "print(\"âœ… Statistical analysis report saved to results/statistical_analysis.txt\")\n",
    "print(\"\\n\".join(report_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸŽ‰ Experiment completed!\")\n",
    "print(\"\\nGenerated files:\")\n",
    "print(\"  - results/baseline_comparison.csv\")\n",
    "print(\"  - results/ablation_rag_threshold.csv\")\n",
    "print(\"  - results/ablation_top_k.csv\")\n",
    "print(\"  - results/ablation_temperature.csv\")\n",
    "print(\"  - results/statistical_analysis.txt\")\n",
    "print(\"  - results/plots/baseline_comparison.png\")\n",
    "print(\"  - results/plots/ablation_threshold.png\")\n",
    "print(\"\\nThese results can be used for paper writing!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_teamwork",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
