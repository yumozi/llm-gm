# RAG Experiment Guide

This directory contains a complete experimental framework for evaluating RAG system performance.

## ğŸ“ File Structure

```
experiments/
â”œâ”€â”€ rag_experiments.ipynb    # Main experiment Notebook â­
â”œâ”€â”€ config.py                # Configuration file
â”œâ”€â”€ test_scenarios.json      # Test scenarios
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ utils/                   # Utility modules
â”‚   â”œâ”€â”€ rag_simulator.py    # RAG simulator
â”‚   â””â”€â”€ metrics.py          # Metrics calculation
â”œâ”€â”€ results/                 # Experiment results (generated after running)
â”‚   â”œâ”€â”€ baseline_comparison.csv
â”‚   â”œâ”€â”€ ablation_*.csv
â”‚   â”œâ”€â”€ statistical_analysis.txt
â”‚   â””â”€â”€ plots/
â””â”€â”€ README.md               # This file
```

## ğŸš€ Quick Start

### 1. Activate Conda Environment

```bash
conda activate llm_teamwork
```

### 2. Install Dependencies

```bash
cd experiments
pip install -r requirements.txt
```

### 3. Run Jupyter Notebook

```bash
jupyter notebook rag_experiments.ipynb
```

Or open `rag_experiments.ipynb` in VS Code

### 4. Execute All Cells Sequentially

The Notebook will automatically:
- Connect to Supabase database
- Load "RAG Test World"
- Run all experiments
- Generate result files and plots

## ğŸ“Š Experiment Contents

### 1. Baseline Comparison

- **No RAG**: Pass all world entities
- **Random Sampling**: Randomly select k entities
- **RAG**: Semantic similarity retrieval

**Output**: `results/baseline_comparison.csv`

### 2. Ablation Studies

#### 2.1 RAG Threshold Ablation
Test similarity thresholds: 0.5, 0.65, 0.8

**Output**: `results/ablation_rag_threshold.csv`

#### 2.2 TOP_K Ablation
Test retrieval counts: 3, 5, 10

**Output**: `results/ablation_top_k.csv`

#### 2.3 Temperature Ablation
Test generation temperatures: 0.5, 0.8, 1.0

**Output**: `results/ablation_temperature.csv`

### 3. Statistical Analysis
- T-test significance testing
- 95% confidence intervals
- Cost efficiency calculations

**Output**: `results/statistical_analysis.txt`

### 4. Visualization
- Baseline comparison bar charts
- Ablation experiment line charts
- Token distribution box plots

**Output**: `results/plots/*.png`

## âš™ï¸ Configuration

### Modify Experiment Configuration

Edit `config.py`:

```python
# Number of runs per configuration
RUNS_PER_CONFIG = 3  # Change to 5 for more stable results

# RAG parameters
RAG_THRESHOLD_VALUES = [0.5, 0.65, 0.8]
TOP_K_VALUES = [3, 5, 10]
TEMPERATURE_VALUES = [0.5, 0.8, 1.0]
```

### Modify Test Scenarios

Edit `test_scenarios.json` to add more test scenarios.

## ğŸ“ˆ Result Interpretation

### Key Metrics

| Metric | Description |
|------|------|
| `input_tokens` | Number of tokens input to GPT-4 |
| `output_tokens` | Number of tokens generated by GPT-4 |
| `latency` | Response latency (seconds) |
| `cost` | API call cost (USD) |
| `entities_retrieved` | Number of entities retrieved |

### Context Efficiency

```
Context Efficiency = (No_RAG_Tokens - RAG_Tokens) / No_RAG_Tokens Ã— 100%
```

Expected result: ~75% reduction

### Statistical Significance

- p < 0.05: Significant difference âœ…
- p â‰¥ 0.05: Not significant âŒ

## ğŸ”§ Troubleshooting

### Issue: Cannot find "RAG Test World"

**Solution**: Ensure "RAG Test World" exists in Supabase database

```sql
SELECT id, name FROM worlds WHERE name = 'RAG Test World';
```

### Issue: OpenAI API Error

**Solution**: Check API key and quota

```bash
# Test API
curl https://api.openai.com/v1/models \
  -H "Authorization: Bearer YOUR_API_KEY"
```

### Issue: match_* function does not exist

**Solution**: Ensure database migration has been executed

```bash
# Execute in Supabase Dashboard SQL Editor
# migrations/022_rag_vector_search.sql
```

### Issue: Experiment runs too slowly

**Solution**: Reduce test scenarios or number of runs

```python
# In Notebook
ablation_scenarios = test_scenarios[:3]  # Use only first 3 scenarios
RUNS_PER_CONFIG = 2  # Reduce to 2 runs
```

## ğŸ’¡ Usage Tips

### 1. Execute Step by Step

Don't run all cells at once, recommended approach:
1. First run baseline comparison
2. Check if results are reasonable
3. Then run ablation experiments

### 2. Save Intermediate Results

Add at key steps:
```python
import pickle
with open('checkpoint.pkl', 'wb') as f:
    pickle.dump(baseline_results, f)
```

### 3. Monitor Cost in Real-time

```python
total_cost = sum([r['input_tokens'] * 0.03/1000 + r['output_tokens'] * 0.06/1000
                  for r in all_results])
print(f"Total cost so far: ${total_cost:.2f}")
```

## ğŸ“ Using Results for Papers

### 1. LaTeX Tables

Use `results/*.csv` files to generate tables:

```python
import pandas as pd

df = pd.read_csv('results/baseline_comparison.csv')
print(df.groupby('config').mean().to_latex(float_format="%.2f"))
```

### 2. Plots

All plots are saved in `results/plots/` directory and can be directly inserted into papers:

```latex
\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{experiments/results/plots/baseline_comparison.png}
\caption{Baseline Comparison Results}
\label{fig:baseline}
\end{figure}
```

### 3. Statistical Data

Copy key data from `results/statistical_analysis.txt` to paper.

## ğŸ¯ Experiment Checklist

Before running experiments, confirm:

- [ ] Conda environment activated (`llm_teamwork`)
- [ ] Dependencies installed (`pip install -r requirements.txt`)
- [ ] Supabase connection working
- [ ] OpenAI API key valid
- [ ] "RAG Test World" exists
- [ ] `match_*` functions created

After running experiments, confirm:

- [ ] `results/` directory created
- [ ] CSV files generated
- [ ] Plots saved
- [ ] Statistical analysis report generated

## ğŸ“ Need Help?

If you encounter problems:
1. Check error output in Notebook
2. Review the troubleshooting section in this README
3. Check if configurations in `config.py` are correct
4. Verify database connection and data integrity

## âš ï¸ Important Notes

1. **Don't modify existing code**: All experiment code is in `experiments/` directory and won't affect main project
2. **API costs**: Complete experiments will cost approximately $5-10 (depending on world size and number of scenarios)
3. **Runtime**: Complete experiments take approximately 30-60 minutes
4. **Result reproducibility**: Each run will produce slightly different results, this is normal (LLM randomness)

## ğŸ”„ Restore Original Code

If you want to delete experiment files:

```bash
cd ..
rm -rf experiments/
```

This won't affect any main project files!

---

**Good luck with your experiments! If results meet expectations, remember to update your paper!** ğŸ‰
